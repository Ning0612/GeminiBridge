# GeminiBridge

[![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-009688.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**GeminiBridge** is an OpenAI API-compatible proxy server that bridges the gap between applications built for OpenAI's API and Google's Gemini models. It translates OpenAI-formatted chat completion requests into Gemini CLI commands, enabling seamless integration without code changes.

## âœ¨ Features

- ğŸ”„ **OpenAI API Compatibility** - Drop-in replacement for OpenAI's chat completion endpoints
- ğŸš€ **High Performance** - Async architecture with configurable concurrency control
- ğŸ”’ **Enterprise Security** - Bearer token authentication, rate limiting, and request validation
- ğŸ“Š **Production Ready** - Structured JSON logging, automatic retry logic, and health monitoring
- ğŸ³ **Docker Support** - Sandboxed CLI execution with automatic conflict resolution
- ğŸŒ **CORS Enabled** - Cross-origin resource sharing for web applications
- âš¡ **Streaming Support** - Server-sent events (SSE) for real-time responses
- ğŸ“ **Comprehensive Logging** - Daily rotation, automatic cleanup, and sensitive data masking

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â”‚ Application â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ OpenAI API Format
       â”‚ (HTTP/JSON)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GeminiBridge Server          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Authentication Middleware    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Rate Limiting Middleware     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     Request Validator          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      Queue Manager             â”‚  â”‚
â”‚  â”‚  (Concurrency Control)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     Gemini CLI Adapter         â”‚  â”‚
â”‚  â”‚  (with Retry Logic)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Gemini CLI Protocol
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Gemini CLI    â”‚
       â”‚  (Sandboxed)    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Gemini Models  â”‚
       â”‚   (Google AI)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+** installed
- **Gemini CLI** installed and configured ([Installation Guide](https://geminicli.com/))
- **Docker** (required for sandboxed execution)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/GeminiBridge.git
   cd GeminiBridge
   ```

2. **Create and activate virtual environment**
   ```bash
   # Windows
   python -m venv .venv
   .venv\Scripts\activate

   # Linux/Mac
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   ```bash
   # Copy example configuration
   # Windows:
   copy .env.example .env
   # Linux/Mac:
   cp .env.example .env

   # Generate secure bearer token
   python scripts/generate_token.py

   # Edit .env and add your token
   # Update GEMINI_CLI_PATH if needed
   ```

5. **Run security check**
   ```bash
   python scripts/check_security.py
   ```

6. **Start the server**
   ```bash
   python main.py
   ```

The server will start on `http://127.0.0.1:11434` by default.

## ğŸ“– Usage

### Basic Example

```python
import requests

url = "http://127.0.0.1:11434/v1/chat/completions"
headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer YOUR_TOKEN_HERE"
}
data = {
    "model": "gpt-4",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
```

### Using OpenAI Python Library

```python
from openai import OpenAI

# Point to GeminiBridge server
client = OpenAI(
    api_key="YOUR_TOKEN_HERE",
    base_url="http://127.0.0.1:11434/v1"
)

# Use exactly like OpenAI API
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Hello, how are you?"}
    ]
)

print(response.choices[0].message.content)
```

### Streaming Responses

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_TOKEN_HERE",
    base_url="http://127.0.0.1:11434/v1"
)

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## ğŸ”‘ Model Mapping

GeminiBridge automatically maps OpenAI model names to Gemini models:

| OpenAI Model | Gemini Model |
|--------------|--------------|
| `gpt-3.5-turbo` | `gemini-2.5-flash` |
| `gpt-3.5-turbo-16k` | `gemini-2.5-flash` |
| `gpt-4` | `gemini-2.5-pro` |
| `gpt-4-turbo` | `gemini-2.5-pro` |
| `gpt-4-turbo-preview` | `gemini-2.5-pro` |
| `gpt-4o` | `gemini-2.5-pro` |
| `gpt-4o-mini` | `gemini-2.5-flash` |

You can also directly request Gemini models:
```json
{
    "model": "gemini-2.5-pro",
    "messages": [...]
}
```

Model mappings are configured in `config/models.json`.

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `PORT` | Server port | `11434` | No |
| `HOST` | Server host | `127.0.0.1` | No |
| `BEARER_TOKEN` | Authentication token | - | **Yes** |
| `GEMINI_CLI_PATH` | Path to Gemini CLI | `gemini` | No |
| `GEMINI_CLI_TIMEOUT` | CLI timeout (seconds) | `30` | No |
| `CLI_MAX_RETRIES` | Max retry attempts for Docker conflicts | `3` | No |
| `MAX_CONCURRENT_REQUESTS` | Max concurrent CLI processes | `5` | No |
| `QUEUE_TIMEOUT` | Queue timeout (seconds) | `30` | No |
| `RATE_LIMIT_MAX_REQUESTS` | Max requests per window | `100` | No |
| `RATE_LIMIT_WINDOW_SECONDS` | Rate limit window | `60` | No |
| `LOG_LEVEL` | Logging level | `INFO` | No |
| `LOG_RETENTION_DAYS` | Log retention period | `7` | No |
| `DEBUG` | Enable debug mode | `false` | No |

See [`.env.example`](.env.example) for detailed configuration options.

**Note**: CORS is currently hardcoded in the application. To customize CORS settings, modify `src/app.py`.

### Security Best Practices

1. **Generate a strong bearer token**
   ```bash
   python scripts/generate_token.py
   ```

2. **Run security checks**
   ```bash
   python scripts/check_security.py
   ```

3. **Use HTTPS in production** - Deploy behind a reverse proxy with TLS enabled

## ğŸ“Š API Endpoints

### Health Check
```http
GET /health
```
Returns server status and queue statistics.

**Response:**
```json
{
    "status": "healthy",
    "service": "GeminiBridge Python",
    "version": "2.0.0",
    "queue": {
        "active_requests": 2,
        "queued_requests": 0,
        "total_processed": 1523,
        "average_wait_time_ms": 45,
        "max_concurrent": 5
    }
}
```

### List Models
```http
GET /v1/models
Authorization: Bearer YOUR_TOKEN
```
Returns list of available models.

### Chat Completions
```http
POST /v1/chat/completions
Authorization: Bearer YOUR_TOKEN
Content-Type: application/json
```

**Request Body:**
```json
{
    "model": "gpt-4",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    "stream": false,
    "temperature": 0.7,
    "max_tokens": 1000
}
```

**Response:**
```json
{
    "id": "chatcmpl-123",
    "object": "chat.completion",
    "created": 1677652288,
    "model": "gpt-4",
    "choices": [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": "Hello! How can I help you today?"
        },
        "finish_reason": "stop"
    }]
}
```

See [API Documentation](docs/API.md) for complete API reference.

## ğŸ› ï¸ Development

### Project Structure

```
GeminiBridge/
â”œâ”€â”€ main.py                 # Application entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment configuration template
â”œâ”€â”€ CLAUDE.md              # AI assistant guidance
â”œâ”€â”€ config/
â”‚   â””â”€â”€ models.json        # Model mapping configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py             # FastAPI application
â”‚   â”œâ”€â”€ config.py          # Configuration loader
â”‚   â”œâ”€â”€ gemini_cli.py      # Gemini CLI adapter
â”‚   â”œâ”€â”€ queue_manager.py   # Concurrency control
â”‚   â”œâ”€â”€ logger.py          # Logging system
â”‚   â””â”€â”€ prompt_builder.py  # Prompt formatter
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_token.py  # Token generator
â”‚   â””â”€â”€ check_security.py  # Security checker
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INDEX.md           # Documentation index
â”‚   â”œâ”€â”€ API.md             # API documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md    # Architecture guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md      # Deployment guide
â”‚   â”œâ”€â”€ SECURITY.md        # Security documentation
â”‚   â””â”€â”€ DEVELOPMENT.md     # Development guide
â””â”€â”€ logs/                  # Generated log files
```

### Running Tests

```bash
# Syntax check
python -m py_compile src/app.py

# Compile all source files
python -m compileall src/

# Security audit
pip install pip-audit
pip-audit

# Test endpoints
curl http://127.0.0.1:11434/health
```

### Code Quality

```bash
# Format code
pip install black
black src/

# Type checking
pip install mypy
mypy src/

# Linting
pip install ruff
ruff check src/
```

## ğŸ³ Docker Deployment

### Using Docker Compose

```yaml
version: '3.8'
services:
  geminibridge:
    build: .
    ports:
      - "11434:11434"
    environment:
      - BEARER_TOKEN=${BEARER_TOKEN}
      - GEMINI_CLI_PATH=/usr/local/bin/gemini
    volumes:
      - ./logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro
```

See [Deployment Guide](docs/DEPLOYMENT.md) for detailed deployment instructions.

## ğŸ“š Documentation

- [API Reference](docs/API.md) - Complete API documentation
- [Architecture Guide](docs/ARCHITECTURE.md) - System architecture and design
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment instructions
- [Security Guide](docs/SECURITY.md) - Security best practices and guidelines
- [Development Guide](docs/DEVELOPMENT.md) - Development workflow and guidelines

## ğŸ”’ Security Features

- **Bearer Token Authentication** - Timing-safe token comparison
- **Rate Limiting** - Per-IP sliding window rate limiting
- **Request Validation** - Comprehensive input validation and sanitization
- **Sandboxed Execution** - Docker-based CLI isolation
- **Sensitive Data Masking** - Automatic masking in logs
- **CORS Protection** - Configurable cross-origin resource sharing
- **DoS Protection** - Request size limits and queue timeouts

See [Security Documentation](docs/SECURITY.md) for detailed security information.

## ğŸ› Troubleshooting

### Common Issues

**CLI Execution Errors**
- Verify `GEMINI_CLI_PATH` points to the correct executable
- Check Docker is running for sandboxed execution
- Review logs in `logs/` directory

**Authentication Failures**
- Ensure `BEARER_TOKEN` matches between client and server
- Check token is properly set in Authorization header

**Performance Issues**
- Increase `MAX_CONCURRENT_REQUESTS` for higher throughput
- Adjust `QUEUE_TIMEOUT` for long-running requests
- Monitor queue statistics via `/health` endpoint

**For more help**, check the server logs or open an issue on GitHub.

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [Google Generative AI](https://ai.google.dev/) - Gemini models
- [OpenAI](https://openai.com/) - API specification

## ğŸ“ Support

- ğŸ› Issues: [GitHub Issues](https://github.com/yourusername/GeminiBridge/issues)

---

**Made with â¤ï¸ by the GeminiBridge Team**
